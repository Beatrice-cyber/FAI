{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b485358",
   "metadata": {},
   "source": [
    "# Week 7 - Online Learning (With Solutions)\n",
    "\n",
    "Welcome to this comprehensive tutorial on creating a Logistic Regression model from scratch! In this notebook, you will embark on an enlightening journey through the fundamental concepts and inner workings of online learning through Logistic Regression, a widely used algorithm in the realm of machine learning and classification tasks. \n",
    "\n",
    "## Authors\n",
    "- Hossein A. Rahmani (hossein.rahmani.22@ucl.ac.uk)\n",
    "- Sahan Bulathwela (m.bulathwela@ucl.ac.uk)\n",
    "\n",
    "## Learning Outcomes\n",
    "- **Understanding Logistic Regression:** Delve into the theoretical underpinnings of Logistic Regression and gain clarity on how it's used for binary classification problems. Learn about the sigmoid function, which is at the heart of this algorithm, and discover its significance in converting linear outputs into probabilistic predictions.\n",
    "- **Comparison with Libraries:** Gain insights into how your scratch-built Logistic Regression model compares with established machine learning libraries like scikit-learn. Understand the underlying similarities and differences, and appreciate the convenience that libraries bring to real-world projects.\n",
    "- **Utilise Different Online Learning Algorithms:** Familiarise with using online learning algorithms available in popular machine learning libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd84d3b",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Classification using Logistic Regression and other online learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c890fd",
   "metadata": {},
   "source": [
    "## Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a781f5",
   "metadata": {},
   "source": [
    "Sources: \n",
    "- https://www.analyticsvidhya.com/blog/2022/02/implementing-logistic-regression-from-scratch-using-python/\n",
    "- https://developer.ibm.com/articles/implementing-logistic-regression-from-scratch-in-python/\n",
    "- https://github.com/AssemblyAI-Examples/Machine-Learning-From-Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e80450",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1cda9",
   "metadata": {},
   "source": [
    "We first import the necessary libraries to implement a logistic regression model, improt dataset, and evalute the model using the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c90afa8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy import log, dot, e, shape\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# to split dataset into train and test parts\n",
    "from sklearn.model_selection import train_test_split\n",
    "# to load dataset\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42890a85",
   "metadata": {},
   "source": [
    "### Logistic Regression: Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa91217",
   "metadata": {},
   "source": [
    "#### Initializing Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70dacaa",
   "metadata": {},
   "source": [
    "To embark on logistic regression modeling, initializing key parameters is the starting point. Four pivotal variables demand specific definitions within the input framework:\n",
    "\n",
    "1. **Learning Rate**: This critical factor governs the step magnitude taken in each optimization iteration. It significantly impacts the model's convergence speed and accuracy.\n",
    "\n",
    "2. **Number of Iterations**: Defining this variable establishes how frequently the optimization algorithm refines the model through multiple passes over the training data. It plays a role in achieving convergence and fine-tuning the model.\n",
    "\n",
    "Moreover, two essential model components must be set:\n",
    "\n",
    "3. **Weights**: These coefficients quantify feature influence in the logistic regression model. They shape predictions by assigning importance to each feature.\n",
    "\n",
    "4. **Bias**: The bias term introduces an offset that aids the model in capturing inherent nuances or noise in the data.\n",
    "\n",
    "In summary, the learning rate, number of iterations, weights, and bias constitute the foundation for launching logistic regression, facilitating accurate and effective modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44aeecd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# initialse the required variables for logestic regression parameters\n",
    "def init(learning_rate, number_iters):\n",
    "    lr = learning_rate\n",
    "    max_ite = number_iters\n",
    "    weights = []\n",
    "    Bias = []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e655122b",
   "metadata": {},
   "source": [
    "#### Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9968828",
   "metadata": {},
   "source": [
    "In a linear regression model, the hypothesis function is a linear combination of parameters given as $\\hat{y} = wx + b$ for a simple single parameter data. This allows us to predict continuous values effectively to find the best fitting line on the dataset, but in logistic regression, the response variables are binomial, either ‘yes’ or ‘no’. So, it makes less sense to use the linear function to predict anything except the values between 0 and 1. And the most effective function to limit the results of a linear equation to [0,1] is the `sigmoid` or `logistic` function. In Logistic Regression, we try to create probabilities instead of a specific value, which makes it suitable for classification problem. To do this, we put the values into a sigmoid function to get the probabilities over the variables:\n",
    "\n",
    "$s(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "Finally, we have:\n",
    "\n",
    "$\\hat{y} = h_{\\theta}(x) = \\frac{1}{1+e^{-wx+b}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1005074f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# here x is equal to wx+b\n",
    "def sigmoid(x):\n",
    "    sig = 1/(1+e**(-x))\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25390c51",
   "metadata": {},
   "source": [
    "#### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964c1ac",
   "metadata": {},
   "source": [
    "The cost function, also known as the `loss function`, defines the extent of disparity between the computed and real values. In linear regression, the least squared error is utilized as the cost function. However, in logistic regression, the least squared error function becomes non-convex, introducing a higher likelihood of gradient descent becoming trapped in local minima. To address this, the preferred choice is to employ the `log loss function` as the cost function. The formula gives the cost function for the logistic regression:\n",
    "\n",
    "$J(w, b) = J(\\theta) = \\frac{1}{N}\\sum_{i=1}^{n}[{y^ilog(h_\\theta(x^i))}+(1-y^{i})log(1-h_\\theta(x^i))]$\n",
    "\n",
    "And the gradianets in terms of wights and bias are:\n",
    "\n",
    "$J^{'}(\\theta)=\\begin{bmatrix} \\frac{dJ}{dw} \\\\ \\frac{dJ}{db} \\end{bmatrix}=\\begin{bmatrix} ... \\end{bmatrix}=\\begin{bmatrix} \\frac{1}{N}\\sum2x_i(\\hat{y}-y_i) \\\\ \\frac{1}{N}\\sum{x_i}(\\hat{y}-y_i) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68bf383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    # loading variables\n",
    "    n_samples, n_features = X.shape\n",
    "    self.weights = np.zeros(n_features)\n",
    "    self.bias = 0\n",
    "\n",
    "    for _ in tqdm(range(self.n_iters)):\n",
    "        # making prediction\n",
    "        linear_pred = np.dot(X, self.weights) + self.bias\n",
    "        predictions = sigmoid(linear_pred)\n",
    "\n",
    "        # Cost functions\n",
    "        # calculating errors: cross entropy\n",
    "        dw = (1/n_samples) * np.dot(X.T,(predictions -y ))\n",
    "        db = (1/n_samples) * np.sum(predictions-y)\n",
    "        \n",
    "        # updating weights and bias\n",
    "        self.weights = self.weights - self.lr * dw\n",
    "        self.bias = self.bias - self.lr * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f4b2e2",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7793c2e",
   "metadata": {},
   "source": [
    "Everything that we have done far is for this step. We trained the model on a training dataset, and now we will use the learned parameters to predict the unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72a5385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    linear_pred = np.dot(X, self.weights) + self.bias\n",
    "    y_pred = sigmoid(linear_pred)\n",
    "    class_pred = [0 if y<=0.5 else 1 for y in y_pred]\n",
    "    return class_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516a321",
   "metadata": {},
   "source": [
    "#### Putting Everything Together: Logistic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c771151",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, lr=0.001, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in tqdm(range(self.n_iters)):\n",
    "            linear_pred = np.dot(X, self.weights) + self.bias\n",
    "            predictions = sigmoid(linear_pred)\n",
    "\n",
    "            # calculating errors: cross entropy\n",
    "            dw = (1/n_samples) * np.dot(X.T,(predictions -y ))\n",
    "            db = (1/n_samples) * np.sum(predictions-y)\n",
    "            \n",
    "            self.weights = self.weights - self.lr * dw\n",
    "            self.bias = self.bias - self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_pred = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = sigmoid(linear_pred)\n",
    "        class_pred = [0 if y<=0.5 else 1 for y in y_pred]\n",
    "        return class_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e5710b",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86096005",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465b1ff",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e1e944b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# evaluation metric\n",
    "def accuracy(y_pred, y_test):\n",
    "    return np.sum(y_pred==y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ca1161",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 1000/1000 [00:00<00:00, 11158.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9298245614035088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "clf = LogisticRegression(lr=0.0001)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "acc = accuracy(y_pred, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29f2539",
   "metadata": {},
   "source": [
    "### Logistic Regression using `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fce4cf3",
   "metadata": {},
   "source": [
    "Now, let’s see how our logistic regression fares in comparison to sklearn’s logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f759799",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/10000 [00:00<?, ?it/s]/var/folders/10/6jmhwzl14wv794rx66ykzdyw0000gn/T/ipykernel_77100/2276547314.py:3: RuntimeWarning: overflow encountered in power\n",
      "  sig = 1/(1+e**(-x))\n",
      "100%|██████████████████████████████████| 10000/10000 [00:00<00:00, 12346.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(n_iters=10000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3cd4e",
   "metadata": {},
   "source": [
    "According to sklearn's Logistic source code, the solver used to minimize the loss function is the SAG solver (Stochastic Average Gradient). This paper defines this method, and in this link there is the implementation of the sag solver. This implementation of the solver uses a method to obtain the step size (learning rate), so there is not a way that you can change the learning rate (unless you want to change the source code).\n",
    "\n",
    "https://datascience.stackexchange.com/questions/16751/learning-rate-in-logistic-regression-with-sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f0ac4",
   "metadata": {},
   "source": [
    "## Running Logistic Regression with Stochastic Gradient Descent\n",
    "\n",
    "Now, let us use the stochastic gradient descent algorithm in the `scikit-learn` library to learn the parameters of the stochastic gradient descent. \n",
    "\n",
    "***Implement logistic regression using the [`SGDClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) class***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6de754d-76dc-438d-8188-35d713a5f18f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9122807017543859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/10/6jmhwzl14wv794rx66ykzdyw0000gn/T/ipykernel_77100/2276547314.py:3: RuntimeWarning: overflow encountered in power\n",
      "  sig = 1/(1+e**(-x))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf_logit = SGDClassifier(loss='log_loss', alpha=0.001, max_iter=10000)\n",
    "clf_logit.fit(X_train, y_train)\n",
    "y_pred_logit = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_logit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6cc897-4315-433d-ad9d-b720c9fc6a98",
   "metadata": {},
   "source": [
    "***Now, implement the perceptron classifier using the same [`SGDClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) class***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d12c1ee8-e66f-4ab7-93e2-20e8c265233d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Accuracy: 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Create the Perceptron classifier\n",
    "clf_perceptron = SGDClassifier(loss='perceptron', penalty=None, learning_rate='constant', eta0=0.1, max_iter=1000)\n",
    "# Train the classifier\n",
    "clf_perceptron.fit(X_train, y_train)\n",
    "# Predict on test data\n",
    "y_pred_perceptron = clf_perceptron.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"Perceptron Accuracy:\", accuracy_score(y_test, y_pred_perceptron))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd919e3-3cb4-47d5-bf68-f477a0a0ae94",
   "metadata": {},
   "source": [
    "### Doing more online learning\n",
    "\n",
    "The whole point of doing online learning is to be able to learn incrementally with new data. Let us try this with the data that we already have. \n",
    "\n",
    "In order to have an online learning setting, we need an addtional \"train\" dataset that the model hasn't seen already. We can populate an addtional training dataset by splitting the test data by half. \n",
    "\n",
    "***Let's split the test data into two splits, namely, 1) `(X_train_delta, y_train_delta)` and  `(X_test, y_test)`.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b768381-c389-42d1-8e5c-4de3e489d83f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_delta,X_test,y_train_delta, y_test = train_test_split(X_test, y_test,train_size = 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98dfb8b-59e5-4720-b24a-c73bed8bda36",
   "metadata": {},
   "source": [
    "***Now let us use the previously trained logistic regression and precetron models to evalaute accuracy once again on the new test set.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a63540f-1e23-4848-913e-fe7fd8da9920",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Logistic Regression Model on new test set is: 0.9298245614035088\n",
      "The accuracy of Perceptron Model on new test set is: 0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of Logistic Regression Model on new test set is: {}\".format(accuracy_score(y_test, clf_logit.predict(X_test))))\n",
    "print(\"The accuracy of Perceptron Model on new test set is: {}\".format(accuracy_score(y_test, clf_perceptron.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739f8bf-213f-43c4-b546-585f99080e5f",
   "metadata": {},
   "source": [
    "In this step, we further train the models with the new training dataset `(X_train_delta, y_train_delta)`. \n",
    "\n",
    "***Let's further train the two models using the new training data.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3229df1d-29b0-484b-8509-d4ae9d6463ef",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070bfd4b-7000-4247-99f3-625788bb47b4",
   "metadata": {},
   "source": [
    "***Now, let us reevaluate the models with the same test dataset that we used before.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91380a28-9a3c-4a63-8489-7ac0e7924454",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"The accuracy of Logistic Regression Model on new test set is: {}\".format(accuracy_score(y_test, clf_logit.predict(X_test))))\n",
    "print(\"The accuracy of Perceptron Model on new test set is: {}\".format(accuracy_score(y_test, clf_perc.predict(X_test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
