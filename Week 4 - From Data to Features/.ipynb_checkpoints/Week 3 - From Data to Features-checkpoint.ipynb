{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b485358",
   "metadata": {},
   "source": [
    "# From Data to Features (Without Solutions)\n",
    "This notebook includes the preprocessing steps for text (in Natural Language Processing) and images (in Computer Vision). Preprocessing involves the steps to transform the texts and images into a clean and consistent format that can be fed into an AI/ML model.\n",
    "\n",
    "## Authors\n",
    "- Hossein A. Rahmani (hossein.rahmani.22@ucl.ac.uk)\n",
    "- Xiao Fu (xiao.fu.20@ucl.ac.uk)\n",
    "\n",
    "## Learning Outcomes\n",
    "- **Fundamental Concepts:** Gain a deep understanding of the foundational concepts in text and image processing, including tokenization, feature extraction, image representation, and manipulation.\n",
    "- **Data Preprocessing:** Learn how to preprocess textual data, including tasks such as text cleaning, tokenization, stopword removal, and stemming/lemmatization. Understand the importance of data preprocessing in improving model performance.\n",
    "- **Image Data Representation:** Learn different methods to represent image data as numerical arrays, including grayscale and color image representations, image normalization, and resizing.\n",
    "- **Hands-on Programming Skills:** Develop proficiency in using Python libraries such as NLTK, scikit-learn, TensorFlow, and Keras to implement text and image processing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd84d3b",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "The notebook will then demonstrate how to implement some text and image preprocssing techniques for machine learning and AI applicaitons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccabf33",
   "metadata": {},
   "source": [
    "## Text Processing in Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa16309",
   "metadata": {},
   "source": [
    "Source:\n",
    "- https://towardsdatascience.com/text-preprocessing-in-natural-language-processing-using-python-6113ff5decd8\n",
    "- https://github.com/Shubha23/Text-processing-NLP/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c5c09",
   "metadata": {},
   "source": [
    "Here, we introduce the text preprocessing steps and implement them using well-known Python libraries such as NLTK. In this notebook, we consider the following steps to preprocess text and clean them. However, there might be some other steps that could be considered based on the tasks/applications you preprocess the text for. \n",
    "\n",
    "- Applications\n",
    "- Parsing\n",
    "- Tokenization\n",
    "- Normalisation\n",
    "- Stop words removal\n",
    "- Lemmatization\n",
    "- Stemming\n",
    "- Bag-of-Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187759e",
   "metadata": {},
   "source": [
    "### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf7b61",
   "metadata": {},
   "source": [
    "Review Sentiment Class Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d666f52",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import string as st # using for punctuation\n",
    "\n",
    "# for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Generate a basic word cloud \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# to plot image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to include bag of word\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826bcce8",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a010020",
   "metadata": {},
   "source": [
    "For our Text Processing task, we use a review dataset on Amazon products (https://github.com/pycaret/pycaret/blob/master/datasets/amazon.csv). The dataset includes two columns, `review` and `sentiment`. The review shows the user reviews on the producs and the sentiment indicates the polarity of sentiment review (`1` for `positive` and `0` for `negative`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data. Here it is already in .csv format\n",
    "text_data = pd.read_csv('data/review_amazon.csv')\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c3818",
   "metadata": {},
   "source": [
    "### Text Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2167d30c",
   "metadata": {},
   "source": [
    "#### Normalisation (e.g. removing punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37477f3d",
   "metadata": {},
   "source": [
    "We begin by implementing text normalization, a process that encompasses various strategies like eliminating punctuation and converting text to lowercase. Specifically, we initiate the text normalization process by removing punctuation marks from the reviews. This preparation step readies the reviews for subsequent tokenization. To achieve this, we create a function named `remove_func` which serves to strip away punctuation marks. The function is then executed on the review dataset, and the results are appended in a new column dedicated to storing the punctuation-removed reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuations from the text\n",
    "def remove_punct(text):\n",
    "    return (\"\".join([ch for ch in text if ch not in st.punctuation]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57400f2b",
   "metadata": {},
   "source": [
    "Apply ` remove_punct()` to raw reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7961fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48248d",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6a910",
   "metadata": {},
   "source": [
    "Tokenization involves breaking down the text into individual tokens or units, which are then amenable to various preprocessing techniques that contribute to generating a high-quality text input for the model. In our case, we utilize the `word_tokenize` method sourced from the `NLTK` package for this purpose. This method effectively splits the text into distinct tokens, allowing us to proceed with subsequent preprocessing operations.\n",
    "\n",
    "Apply `word_tokenize()` to normalised reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f241ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nltk's word tokenizer\n",
    "# Your Code Here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d47bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b584f16d",
   "metadata": {},
   "source": [
    "#### Stop Words Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5554db",
   "metadata": {},
   "source": [
    "Moving forward, the subsequent task involves eliminating stop words from the tokenized reviews. To achieve this, we leverage the NLTK corpus list, which serves as a reference for identifying stop words. It's worth noting that a tailored and user-defined list of stop words could also be generated and employed, thereby restricting the identification of stop words within the input text to the specific terms listed in the custom list. This approach provides an added layer of control over the stop word removal process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4bcfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999a4922",
   "metadata": {},
   "source": [
    "Implement `remove_stopwords()` where remove all stop words from a list `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d7adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    # Your Code Here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f08aa",
   "metadata": {},
   "source": [
    "Apply ` remove_stopwords()` to tokenized reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31e3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e375fd9",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1032da94",
   "metadata": {},
   "source": [
    "For the purpose of lemmatizing words, we utilize the function provided by WordNet within the NLTK framework. This function proves effective in reducing words to their base or lemma forms. Notably, if a given word is not located within the WordNet database, the function retains the word in its original state without any alteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a57b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining word lemmatizer class\n",
    "word_net = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79918db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying lemmatization on tokens\n",
    "def lemmatize(text):\n",
    "    return [word_net.lemmatize(word) for word in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c07c6",
   "metadata": {},
   "source": [
    "Apply ` lemmatize()` to stop removed reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb035fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14dd3f8",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8fb897",
   "metadata": {},
   "source": [
    "For extracting the word stems, we employ the PorterStemmer algorithm, a component of the NLTK package. This algorithm operates based on the approach outlined by \"Porter, M. \"An algorithm for suffix stripping\" in the Program 14.3 (1980): 130-137\". The PorterStemmer algorithm systematically shortens words to their root forms, facilitating the reduction of words to their essential core. This approach proves valuable in contexts where a more aggressive word reduction technique is desired, compared to lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60391564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorterStemmer class from NLTK\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming to get root words \n",
    "def stemming(text):\n",
    "    return [ps.stem(word) for word in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b2994",
   "metadata": {},
   "source": [
    "Apply ` stemming()` to lemmatized reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5375d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45df4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce7fe01",
   "metadata": {},
   "source": [
    "#### Clean Sentences\n",
    "Implement `return_sentences()` to build sentences from the list `tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eebc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentences to get clean text as input for vectors\n",
    "def return_sentences(tokens):\n",
    "    # Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a490c4",
   "metadata": {},
   "source": [
    "Apply ` return_sentences()` to lemmatized reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d416339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12613e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcf675",
   "metadata": {},
   "source": [
    "#### WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed83e4",
   "metadata": {},
   "source": [
    "A **word cloud** is a visual depiction of text where word frequency is represented through varying font size or color. More frequent words appear larger, offering a quick overview of prominent terms and themes in a text or set of texts. It's a tool for summarizing and visualizing textual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce30260",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join([review for review in text_data['clean_review']])\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud(max_words=100).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f0b92",
   "metadata": {},
   "source": [
    "#### Bag-of-Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d42e12",
   "metadata": {},
   "source": [
    "Here is process the text data and convert it to number to prepare them to fed to ML model as machine can only understand the numbers. The **Bag of Words (BoW):** is a text representation method treating text as a collection of words, focusing on their frequency while disregarding order and grammar. To do this, we use **CountVectorizer()** from the **scikit-learn** package. Scikit-learn is a tool that transforms text into a word count matrix, enabling machine learning by converting text data into numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbba8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "review_BOG = count_vect.fit_transform(text_data['clean_review'])\n",
    "print(\"Type of output: \", type(review_BOG))\n",
    "print(\"Shape of Bag-of-Words: \", review_BOG.shape)\n",
    "print(review_BOG[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09967465",
   "metadata": {},
   "source": [
    "## Image Preprocessing in Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d360c",
   "metadata": {},
   "source": [
    "Source: https://www.section.io/engineering-education/image-preprocessing-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e80450",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import glob\n",
    "# ! pip install scikit-image\n",
    "import skimage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "\n",
    "# import libraries for data augmentation\n",
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced7b95",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d9229",
   "metadata": {},
   "source": [
    "Frist, we set the path for the Animal (https://www.kaggle.com/datasets/vishweshsalodkar/wild-animals?resource=download) dataset that we use for our image preprocessing, then we load all the exsiting images from the selected class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and Loading the data\n",
    "dataset_path = 'data/Animals'\n",
    "class_names = ['Cheetah', 'Jaguar', 'Leopard', 'Lion','Tiger']\n",
    "\n",
    "# apply glob module to retrieve files/pathnames  \n",
    "animal_path = os.path.join(dataset_path, class_names[1], '*') # generating the path of all Jaguar class images\n",
    "animal_path = glob.glob(animal_path) # laoding all Jaguar class images paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a9c7a",
   "metadata": {},
   "source": [
    "#### Showing an exmaple of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532ce41",
   "metadata": {},
   "source": [
    "To confirm the correct loading of our data, we attempt to access an image file within the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72158ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing an image file from the dataset classes\n",
    "image = skimage.io.imread(animal_path[4])\n",
    "\n",
    "# plotting the original image\n",
    "i, (im1) = plt.subplots(1)\n",
    "i.set_figwidth(15)\n",
    "im1.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4854ff8",
   "metadata": {},
   "source": [
    "#### RGB channels of image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070cef6",
   "metadata": {},
   "source": [
    "Prior to delving into preprocessing methods, it's prudent to initially examine the RGB channels present in our original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the original image and the RGB channels  \n",
    "i, (im1, im2, im3, im4) = plt.subplots(1, 4, sharey=True)\n",
    "i.set_figwidth(20) \n",
    "\n",
    "im1.imshow(image)  # Original image\n",
    "im2.imshow(image[:, : , 0]) # Red\n",
    "im3.imshow(image[:, : , 1]) # Green\n",
    "im4.imshow(image[:, : , 2]) # Blue\n",
    "\n",
    "i.suptitle('Original & RGB image channels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49139bb3",
   "metadata": {},
   "source": [
    "### Image Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0d7e3",
   "metadata": {},
   "source": [
    "#### Grayscale Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d01ee11",
   "metadata": {},
   "source": [
    "As color is often unnecessary for image recognition, opting for grayscale can be a prudent choice. This not only trims down the pixel count in an image but also curtails the computational load. To put this into action, run the provided code to convert the initial image to grayscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_image = skimage.color.rgb2gray(image) # change the image color\n",
    "plt.imshow(gray_image, cmap = 'gray') # show image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929784a",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8aaed",
   "metadata": {},
   "source": [
    "Normalization is commonly applied to transform the pixel values of an image into a standard or more recognizable scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_image = (gray_image - np.min(gray_image)) / (np.max(gray_image) - np.min(gray_image))\n",
    "plt.imshow(norm_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb1dff",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d748e0",
   "metadata": {},
   "source": [
    "Data augmentation is the process of making minor alterations to existing data to increase its diversity without collecting new data. It is a technique used for enlarging a dataset. Standard data augmentation techniques include horizontal & vertical flipping, rotation, cropping, shearing, etc. Here, we use `Keras's ImageDataGenerator` class to augment our data. This is because it provides a quick and easy way to augment your images. In addition, it supports augmentation techniques such as flips, rotations, brightness change, etc. Let's now look at the most used data augmentation techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(image, datagen):\n",
    "    # expand dimension to one sample\n",
    "    samples = expand_dims(image, 0)\n",
    "\n",
    "    # create an iterator\n",
    "    it = datagen.flow(samples, batch_size=1)\n",
    "    fig, im = plt.subplots(nrows=1, ncols=3, figsize=(15,15))\n",
    "\n",
    "    # generate batch of images\n",
    "    for i in range(3):\n",
    "        # convert to unsigned integers\n",
    "        image = next(it)[0].astype('uint8')\n",
    "        # plot image\n",
    "        im[i].imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1f9f1",
   "metadata": {},
   "source": [
    "##### Shifting\n",
    "This is the process of shifting image pixels horizontally or vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e95b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image data augmentation generator\n",
    "datagen = ImageDataGenerator(width_shift_range=[-200,200])\n",
    "image = skimage.io.imread(animal_path[4])\n",
    "data_augmentation(image, datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ca072",
   "metadata": {},
   "source": [
    "##### Flipping\n",
    "This reverses the rows or columns of pixels in either vertical or horizontal cases, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator for flipping\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "image = skimage.io.imread(animal_path[4])\n",
    "data_augmentation(image, datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6b902",
   "metadata": {},
   "source": [
    "##### Rotation\n",
    "This process involves rotating an image by a specified degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b22fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range=20, fill_mode='nearest')\n",
    "image = skimage.io.imread(animal_path[4])\n",
    "data_augmentation(image, datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f0ac4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "By engaging with this notebook, you will:\n",
    "\n",
    "- Learn data preprocessing steps for both text and image data.\n",
    "- Acquire skills in feature extraction, classification, and fusion for enhanced model performance.\n",
    "- Understand and experience in Python libraries for real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
